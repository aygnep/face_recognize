{"cells": [{"cell_type": "markdown", "metadata": {"toc-hr-collapsed": false}, "source": ["# \u76ee\u6807\u68c0\u6d4b\uff1a\u53e3\u7f69\u4f69\u6234\u68c0\u6d4b  \n", "\n", "<br>\n", "<hr>"]}, {"cell_type": "markdown", "metadata": {"toc-hr-collapsed": true}, "source": ["## 1.\u5b9e\u9a8c\u4ecb\u7ecd"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.1 \u5b9e\u9a8c\u80cc\u666f  \n", "\n", "\u4eca\u5e74\u4e00\u573a\u5e2d\u5377\u5168\u7403\u7684\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u7ed9\u4eba\u4eec\u5e26\u6765\u4e86\u6c89\u91cd\u7684\u751f\u547d\u8d22\u4ea7\u7684\u635f\u5931\u3002  \n", "\u6709\u6548\u9632\u5fa1\u8fd9\u79cd\u4f20\u67d3\u75c5\u6bd2\u7684\u65b9\u6cd5\u5c31\u662f\u79ef\u6781\u4f69\u6234\u53e3\u7f69\u3002  \n", "\u6211\u56fd\u5bf9\u6b64\u4e5f\u91c7\u53d6\u4e86\u4e25\u8083\u7684\u63aa\u65bd\uff0c\u5728\u516c\u5171\u573a\u5408\u8981\u6c42\u4eba\u4eec\u5fc5\u987b\u4f69\u6234\u53e3\u7f69\u3002  \n", "\u5728\u672c\u6b21\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u8981\u5efa\u7acb\u4e00\u4e2a\u76ee\u6807\u68c0\u6d4b\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u8bc6\u522b\u56fe\u4e2d\u7684\u4eba\u662f\u5426\u4f69\u6234\u4e86\u53e3\u7f69\u3002"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.2 \u5b9e\u9a8c\u8981\u6c42\n", "\n", "1\uff09\u5efa\u7acb\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u68c0\u6d4b\u51fa\u56fe\u4e2d\u7684\u4eba\u662f\u5426\u4f69\u6234\u4e86\u53e3\u7f69\uff0c\u5e76\u5c06\u5176\u5c3d\u53ef\u80fd\u8c03\u6574\u5230\u6700\u4f73\u72b6\u6001\u3002  \n", "2\uff09\u5b66\u4e60\u7ecf\u5178\u7684\u6a21\u578b MTCNN \u548c MobileNet \u7684\u7ed3\u6784\u3002  \n", "3\uff09\u5b66\u4e60\u8bad\u7ec3\u65f6\u7684\u65b9\u6cd5\u3002  \n", "<br>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.3 \u5b9e\u9a8c\u73af\u5883\n", "\n", "\u53ef\u4ee5\u4f7f\u7528\u57fa\u4e8e Python \u7684 OpenCV \u3001PIL \u5e93\u8fdb\u884c\u56fe\u50cf\u76f8\u5173\u5904\u7406\uff0c\u4f7f\u7528 Numpy \u5e93\u8fdb\u884c\u76f8\u5173\u6570\u503c\u8fd0\u7b97\uff0c\u4f7f\u7528 Pytorch \u7b49\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\u6a21\u578b\u7b49\u3002\n", "<br>\n", "<br>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.4 \u6ce8\u610f\u4e8b\u9879  \n", "+ Python \u4e0e Python Package \u7684\u4f7f\u7528\u65b9\u5f0f\uff0c\u53ef\u5728\u53f3\u4fa7 `API\u6587\u6863` \u4e2d\u67e5\u9605\u3002\n", "+ \u5f53\u53f3\u4e0a\u89d2\u7684\u300ePython 3\u300f\u957f\u65f6\u95f4\u6307\u793a\u4e3a\u8fd0\u884c\u4e2d\u7684\u65f6\u5019\uff0c\u9020\u6210\u4ee3\u7801\u65e0\u6cd5\u6267\u884c\u65f6\uff0c\u53ef\u4ee5\u91cd\u65b0\u542f\u52a8 Kernel \u89e3\u51b3\uff08\u5de6\u4e0a\u89d2\u300eKernel\u300f-\u300eRestart Kernel\u300f\uff09\u3002"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.5 \u53c2\u8003\u8d44\u6599\n", "+ \u8bba\u6587 Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks\uff1ahttps://kpzhang93.github.io/MTCNN_face_detection_alignment/\n", "+ OpenCV\uff1ahttps://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_tutorials.html\n", "+ PIL\uff1ahttps://pillow.readthedocs.io/en/stable/\n", "+ Numpy\uff1ahttps://www.numpy.org/\n", "+ Scikit-learn\uff1a https://scikit-learn.org/\n", "+ PyTorch\uff1ahttps://pytorch.org/"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1.6 \u5b9e\u9a8c\u601d\u8def\n", "\n", "\u9488\u5bf9\u76ee\u6807\u68c0\u6d4b\u7684\u4efb\u52a1\uff0c\u53ef\u4ee5\u5206\u4e3a\u4e24\u4e2a\u90e8\u5206\uff1a\u76ee\u6807\u8bc6\u522b\u548c\u4f4d\u7f6e\u68c0\u6d4b\u3002  \n", "\u901a\u5e38\u60c5\u51b5\u4e0b\uff0c\u7279\u5f81\u63d0\u53d6\u9700\u8981\u7531\u7279\u6709\u7684\u7279\u5f81\u63d0\u53d6\u795e\u7ecf\u7f51\u7edc\u6765\u5b8c\u6210\uff0c\u5982 VGG\u3001MobileNet\u3001ResNet \u7b49\uff0c\u8fd9\u4e9b\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u5f80\u5f80\u88ab\u79f0\u4e3a Backbone \u3002\u800c\u5728 BackBone \u540e\u9762\u63a5\u5168\u8fde\u63a5\u5c42(FC)\u5c31\u53ef\u4ee5\u6267\u884c\u5206\u7c7b\u4efb\u52a1\u3002  \n", "\u4f46 FC \u5bf9\u76ee\u6807\u7684\u4f4d\u7f6e\u8bc6\u522b\u4e4f\u529b\u3002\u7ecf\u8fc7\u7b97\u6cd5\u7684\u53d1\u5c55\uff0c\u5f53\u524d\u4e3b\u8981\u4ee5\u7279\u5b9a\u7684\u529f\u80fd\u7f51\u7edc\u6765\u4ee3\u66ff FC \u7684\u4f5c\u7528\uff0c\u5982 Mask-Rcnn\u3001SSD\u3001YOLO \u7b49\u3002  \n", "\u6211\u4eec\u9009\u62e9\u5145\u5206\u4f7f\u7528\u5df2\u6709\u7684\u4eba\u8138\u68c0\u6d4b\u7684\u6a21\u578b\uff0c\u518d\u8bad\u7ec3\u4e00\u4e2a\u8bc6\u522b\u53e3\u7f69\u7684\u6a21\u578b\uff0c\u4ece\u800c\u63d0\u9ad8\u8bad\u7ec3\u7684\u5f00\u652f\u3001\u589e\u5f3a\u6a21\u578b\u7684\u51c6\u786e\u7387\u3002\n", "\n", "**\u5e38\u89c4\u76ee\u6807\u68c0\u6d4b\uff1a**  \n", "\n", "<img src=\"https://imgbed.momodel.cn/20200914162156.png\" width=500px/>\n", "\n", "\n", "\n", "**\u672c\u6b21\u6848\u4f8b\uff1a**   \n", "\n", "\n", "<img src=\"https://imgbed.momodel.cn/20200918102630.png\" width=500px/>\n", "\n", "<br>\n", "<br>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2.\u6570\u636e\u96c6\u4ecb\u7ecd"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.1 \u5bfc\u5165 Python \u7b2c\u4e09\u65b9\u5e93\uff08\u5305\uff09"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "select": true}, "outputs": [], "source": ["import warnings\n", "# \u5ffd\u89c6\u8b66\u544a\n", "warnings.filterwarnings('ignore')\n", "\n", "import cv2\n", "from PIL import Image\n", "import numpy as np\n", "import copy\n", "import matplotlib.pyplot as plt\n", "from tqdm.auto import tqdm\n", "import torch\n", "import torch.nn as nn\n", "import torch.optim as optim\n", "from torchvision.datasets import ImageFolder\n", "import torchvision.transforms as T\n", "from torch.utils.data import DataLoader\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.2 \u5bfc\u5165\u5df2\u7ecf\u5199\u597d\u7684 Python \u6587\u4ef6"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "select": true}, "outputs": [], "source": ["from torch_py.Utils import plot_image\n", "from torch_py.MTCNN.detector import FaceDetector\n", "from torch_py.MobileNetV1 import MobileNetV1\n", "from torch_py.FaceRec import Recognition\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.3 \u6570\u636e\u96c6\u4ecb\u7ecd\n", "\n", "\u6570\u636e\u4fe1\u606f\u5b58\u653e\u5728 `/datasets/5f680a696ec9b83bb0037081-momodel/data` \u6587\u4ef6\u5939\u4e0b\u3002    \n", "\u8be5\u6587\u4ef6\u5939\u4e3b\u8981\u6709\u6587\u4ef6\u5939 `image`\u3001\u6587\u4ef6 `train.txt` \u3001\u6587\u4ef6\u5939 `keras_model_data` \u548c\u6587\u4ef6\u5939 `mindspore_model_data`\u5171\u56db\u90e8\u5206\uff1a\n", "+ **image \u6587\u4ef6\u5939**\uff1a\u56fe\u7247\u5206\u6210\u4e24\u7c7b\uff0c\u6234\u53e3\u7f69\u7684\u548c\u6ca1\u6709\u6234\u53e3\u7f69\u7684  \n", "+ **train.txt**\uff1a  \u5b58\u653e\u7684\u662f image \u6587\u4ef6\u5939\u4e0b\u5bf9\u5e94\u56fe\u7247\u7684\u6807\u7b7e\uff08keras \u7248\u672c\u4f5c\u4e1a\u9700\u8981\u7528\u5230\uff09  \n", "+ **keras_model_data** \u6587\u4ef6\u5939\uff1a\u5b58\u653e keras \u6846\u67b6\u76f8\u5173\u9884\u8bad\u7ec3\u597d\u7684\u6a21\u578b \uff08keras \u7248\u672c\u4f5c\u4e1a\u9700\u8981\u7528\u5230\uff09\n", "+ **mindspore_model_data** \u6587\u4ef6\u5939\uff1a\u5b58\u653e mindspore \u6846\u67b6\u76f8\u5173\u9884\u8bad\u7ec3\u597d\u7684\u6a21\u578b\uff08mindspore \u7248\u672c\u4f5c\u4e1a\u9700\u8981\u7528\u5230\uff09"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u6570\u636e\u96c6\u8def\u5f84\n", "data_path = \"./datasets/5f680a696ec9b83bb0037081-momodel/data/\"\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u73b0\u5728\u6211\u4eec\u5c1d\u8bd5\u8bfb\u53d6\u6570\u636e\u96c6\u4e2d\u6234\u53e3\u7f69\u7684\u56fe\u7247\u53ca\u5176\u540d\u79f0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u4ee5\u4e0b\u662f\u8bad\u7ec3\u96c6\u4e2d\u7684\u6b63\u6837\u672c\uff1a"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mask_num = 4\n", "fig = plt.figure(figsize=(15, 15))\n", "for i in range(mask_num):\n", "    sub_img = cv2.imread(data_path + \"/image/mask/mask_\" + str(i + 101) + \".jpg\")\n", "    sub_img = cv2.cvtColor(sub_img, cv2.COLOR_RGB2BGR)\n", "    ax = fig.add_subplot(4, 4, (i + 1))\n", "    ax.set_xticks([])\n", "    ax.set_yticks([])\n", "    ax.set_title(\"mask_\" + str(i + 1))\n", "    ax.imshow(sub_img)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u4ee5\u4e0b\u662f\u8bad\u7ec3\u96c6\u4e2d\u7684\u8d1f\u6837\u672c\uff1a"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["nomask_num = 4\n", "fig1 = plt.figure(figsize=(15, 15))\n", "for i in range(nomask_num):\n", "    sub_img = cv2.imread(data_path + \"/image/nomask/nomask_\" + str(i + 130) + \".jpg\")\n", "    sub_img = cv2.cvtColor(sub_img, cv2.COLOR_RGB2BGR)\n", "    ax = fig1.add_subplot(4, 4, (i + 1))\n", "    ax.set_xticks([])\n", "    ax.set_yticks([])\n", "    ax.set_title(\"nomask_\" + str(i + 1))\n", "    ax.imshow(sub_img)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.4 \u8c03\u6574\u56fe\u7247\u5c3a\u5bf8"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def letterbox_image(image, size):\n", "    \"\"\"\n", "    \u8c03\u6574\u56fe\u7247\u5c3a\u5bf8\n", "    :param image: \u7528\u4e8e\u8bad\u7ec3\u7684\u56fe\u7247\n", "    :param size: \u9700\u8981\u8c03\u6574\u5230\u7f51\u7edc\u8f93\u5165\u7684\u56fe\u7247\u5c3a\u5bf8\n", "    :return: \u8fd4\u56de\u7ecf\u8fc7\u8c03\u6574\u7684\u56fe\u7247\n", "    \"\"\"\n", "    new_image = cv2.resize(image, size, interpolation=cv2.INTER_AREA)\n", "    return new_image\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u67e5\u770b\u56fe\u7247\u5c3a\u5bf8\u8c03\u6574\u524d\u540e\u7684\u5bf9\u6bd4"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u4f7f\u7528 PIL.Image \u8bfb\u53d6\u56fe\u7247\n", "read_img = Image.open(\"test1.jpg\")\n", "read_img = np.array(read_img)\n", "print(\"\u8c03\u6574\u524d\u56fe\u7247\u7684\u5c3a\u5bf8:\", read_img.shape)\n", "read_img = letterbox_image(image=read_img, size=(50, 50))\n", "read_img = np.array(read_img)\n", "print(\"\u8c03\u6574\u524d\u56fe\u7247\u7684\u5c3a\u5bf8:\", read_img.shape)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.5 \u5236\u4f5c\u8bad\u7ec3\u65f6\u6240\u9700\u7684\u6279\u91cf\u6570\u636e\u96c6"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Pytorch** \u8bfb\u53d6\u6570\u636e\u867d\u7136\u7279\u522b\u7075\u6d3b\uff0c\u4f46\u662f\u8fd8\u662f\u5177\u6709\u7279\u5b9a\u7684\u6d41\u7a0b\u7684\uff0c\u5b83\u7684\u64cd\u4f5c\u987a\u5e8f\u4e3a\uff1a\n", "\n", "+ \u521b\u5efa\u4e00\u4e2a `Dataset` \u5bf9\u8c61\uff0c\u8be5\u5bf9\u8c61\u5982\u679c\u73b0\u6709\u7684 `Dataset` \u4e0d\u80fd\u591f\u6ee1\u8db3\u9700\u6c42\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u81ea\u5b9a\u4e49 `Dataset`\uff0c\u901a\u8fc7\u7ee7\u627f `torch.utils.data.Dataset`\u3002\u5728\u7ee7\u627f\u7684\u65f6\u5019\uff0c\u9700\u8981 `override` \u4e09\u4e2a\u65b9\u6cd5\u3002\n", "    + `__init__`\uff1a \u7528\u6765\u521d\u59cb\u5316\u6570\u636e\u96c6\n", "    + `__getitem__`\uff1a\u7ed9\u5b9a\u7d22\u5f15\u503c\uff0c\u8fd4\u56de\u8be5\u7d22\u5f15\u503c\u5bf9\u5e94\u7684\u6570\u636e\uff1b\u5b83\u662fpython built-in\u65b9\u6cd5\uff0c\u5176\u4e3b\u8981\u4f5c\u7528\u662f\u80fd\u8ba9\u8be5\u7c7b\u53ef\u4ee5\u50cflist\u4e00\u6837\u901a\u8fc7\u7d22\u5f15\u503c\u5bf9\u6570\u636e\u8fdb\u884c\u8bbf\u95ee\n", "    + `__len__`\uff1a\u7528\u4e8elen(Dataset)\u65f6\u80fd\u591f\u8fd4\u56de\u5927\u5c0f\n", "+ \u521b\u5efa\u4e00\u4e2a `DataLoader` \u5bf9\u8c61\n", "+ \u4e0d\u505c\u7684 \u5faa\u73af \u8fd9\u4e2a `DataLoader` \u5bf9\u8c61"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 2.5.1 \u7b2c\u4e00\u6b65\uff1a\u521b\u5efa\u4e00\u4e2a `Dataset` \u5bf9\u8c61\n", "\n", "**torchvision.datasets.ImageFolder** \u662f\u4e00\u4e2a\u901a\u7528\u7684\u6570\u636e\u52a0\u8f7d\u5668\uff0c\u5e38\u89c1\u7684\u7528\u6cd5\u5982\u4e0b\uff1a\n", "> dataset=torchvision.datasets.ImageFolder(root, transform=None, target_transform=None, loader=<function default_loader>, is_valid_file=None)\n", "    \n", "+ **\u53c2\u6570\u8be6\u89e3**\uff1a\n", "    + root\uff1a\u56fe\u7247\u5b58\u50a8\u7684\u6839\u76ee\u5f55\uff0c\u5373\u5404\u7c7b\u522b\u6587\u4ef6\u5939\u6240\u5728\u76ee\u5f55\u7684\u4e0a\u4e00\u7ea7\u76ee\u5f55\u3002\n", "    + transform\uff1a\u5bf9\u56fe\u7247\u8fdb\u884c\u9884\u5904\u7406\u7684\u64cd\u4f5c\uff08\u51fd\u6570\uff09\uff0c\u539f\u59cb\u56fe\u7247\u4f5c\u4e3a\u8f93\u5165\uff0c\u8fd4\u56de\u4e00\u4e2a\u8f6c\u6362\u540e\u7684\u56fe\u7247\u3002\n", "    + target_transform\uff1a\u5bf9\u56fe\u7247\u7c7b\u522b\u8fdb\u884c\u9884\u5904\u7406\u7684\u64cd\u4f5c\uff0c\u8f93\u5165\u4e3a target\uff0c\u8f93\u51fa\u5bf9\u5176\u7684\u8f6c\u6362\u3002\u5982\u679c\u4e0d\u4f20\u8be5\u53c2\u6570\uff0c\u5373\u5bf9 target \u4e0d\u505a\u4efb\u4f55\u8f6c\u6362\uff0c\u8fd4\u56de\u7684\u987a\u5e8f\u7d22\u5f15 0,1, 2\u2026\n", "    + loader\uff1a\u8868\u793a\u6570\u636e\u96c6\u52a0\u8f7d\u65b9\u5f0f\uff0c\u901a\u5e38\u9ed8\u8ba4\u52a0\u8f7d\u65b9\u5f0f\u5373\u53ef\u3002\n", "    + is_valid_file\uff1a\u83b7\u53d6\u56fe\u50cf\u6587\u4ef6\u7684\u8def\u5f84\u5e76\u68c0\u67e5\u8be5\u6587\u4ef6\u662f\u5426\u4e3a\u6709\u6548\u6587\u4ef6\u7684\u51fd\u6570(\u7528\u4e8e\u68c0\u67e5\u635f\u574f\u6587\u4ef6)\n", "    \n", "+ \u8fd4\u56de\u7684 dataset \u90fd\u6709\u4ee5\u4e0b\u4e09\u79cd\u5c5e\u6027\uff1a\n", "    + dataset.classes\uff1a\u7528\u4e00\u4e2a list \u4fdd\u5b58\u7c7b\u522b\u540d\u79f0\n", "    + dataset.class_to_idx\uff1a\u7c7b\u522b\u5bf9\u5e94\u7684\u7d22\u5f15\uff0c\u4e0e\u4e0d\u505a\u4efb\u4f55\u8f6c\u6362\u8fd4\u56de\u7684 target \u5bf9\u5e94\n", "    + dataset.imgs\uff1a\u4fdd\u5b58(img-path, class) tuple \u7684\u5217\u8868"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 2.5.2 \u521b\u5efa\u4e00\u4e2a `DataLoader` \u5bf9\u8c61\n", "\n", "`DataLoader` \u662f `torch` \u7ed9\u4f60\u7528\u6765\u5305\u88c5\u4f60\u7684\u6570\u636e\u7684\u5de5\u5177\uff0c\u6240\u4ee5\u4f60\u8981\u5c06( numpy array \u6216\u5176\u4ed6) \u6570\u636e\u5f62\u5f0f\u88c5\u6362\u6210 Tensor, \u7136\u540e\u518d\u653e\u8fdb\u8fd9\u4e2a\u5305\u88c5\u5668\u4e2d\u3002 \u4f7f\u7528 `DataLoader` \u5e2e\u52a9\u6211\u4eec\u5bf9\u6570\u636e\u8fdb\u884c\u6709\u6548\u5730\u8fed\u4ee3\u5904\u7406\u3002\n", "\n", "> torch.utils.data.DataLoader(dataset,batch_size=1,shuffle=False,\n", " sampler=None,\n", " batch_sampler=None,\n", " num_workers=0,\n", " collate_fn=<function default_collate>,\n", " pin_memory=False,\n", " drop_last=False,\n", " timeout=0,\n", " worker_init_fn=None)\n", "    \n", "- **\u5e38\u7528\u53c2\u6570\u89e3\u91ca**\uff1a\n", "    + dataset (Dataset): \u662f\u4e00\u4e2a DataSet \u5bf9\u8c61\uff0c\u8868\u793a\u9700\u8981\u52a0\u8f7d\u7684\u6570\u636e\u96c6\n", "    + batch_size (int, optional): \u6bcf\u4e00\u4e2a batch \u52a0\u8f7d\u591a\u5c11\u7ec4\u6837\u672c\uff0c\u5373\u6307\u5b9a batch_size \uff0c\u9ed8\u8ba4\u662f 1 \n", "    + shuffle (bool, optional): \u5e03\u5c14\u503c True \u6216\u8005\u662f False \uff0c\u8868\u793a\u6bcf\u4e00\u4e2a epoch \u4e4b\u540e\u662f\u5426\u5bf9\u6837\u672c\u8fdb\u884c\u968f\u673a\u6253\u4e71\uff0c\u9ed8\u8ba4\u662f False\n", "\n", "    + sampler (Sampler, optional): \u81ea\u5b9a\u4e49\u4ece\u6570\u636e\u96c6\u4e2d\u62bd\u53d6\u6837\u672c\u7684\u7b56\u7565\uff0c\u5982\u679c\u6307\u5b9a\u8fd9\u4e2a\u53c2\u6570\uff0c\u90a3\u4e48 shuffle \u5fc5\u987b\u4e3a False\n", "    + batch_sampler (Sampler, optional): \u4e0e sampler \u7c7b\u4f3c\uff0c\u4f46\u662f\u4e00\u6b21\u53ea\u8fd4\u56de\u4e00\u4e2a batch \u7684 indices\uff08\u7d22\u5f15\uff09\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4e00\u65e6\u6307\u5b9a\u4e86\u8fd9\u4e2a\u53c2\u6570\uff0c\u90a3\u4e48 batch_size,shuffle,sampler,drop_last \u5c31\u4e0d\u80fd\u518d\u5236\u5b9a\u4e86\uff08\u4e92\u65a5\uff09\n", "\n", "    + num_workers (int, optional): \u8fd9\u4e2a\u53c2\u6570\u51b3\u5b9a\u4e86\u6709\u51e0\u4e2a\u8fdb\u7a0b\u6765\u5904\u7406 data loading \u30020 \u610f\u5473\u7740\u6240\u6709\u7684\u6570\u636e\u90fd\u4f1a\u88ab load \u8fdb\u4e3b\u8fdb\u7a0b\uff0c\u9ed8\u8ba4\u4e3a0\n", "    + collate_fn (callable, optional): \u5c06\u4e00\u4e2a list \u7684 sample \u7ec4\u6210\u4e00\u4e2a mini-batch \u7684\u51fd\u6570\uff08\u8fd9\u4e2a\u8fd8\u4e0d\u662f\u5f88\u61c2\uff09\n", "    + pin_memory (bool, optional): \u5982\u679c\u8bbe\u7f6e\u4e3aTrue\uff0c\u90a3\u4e48 data loader \u5c06\u4f1a\u5728\u8fd4\u56de\u5b83\u4eec\u4e4b\u524d\uff0c\u5c06 tensors \u62f7\u8d1d\u5230 CUDA \u4e2d\u7684\u56fa\u5b9a\u5185\u5b58\uff08CUDA pinned memory\uff09\u4e2d\n", "\n", "    + drop_last (bool, optional): \u5982\u679c\u8bbe\u7f6e\u4e3a True\uff1a\u8fd9\u4e2a\u662f\u5bf9\u6700\u540e\u7684\u672a\u5b8c\u6210\u7684 batch \u6765\u8bf4\u7684\uff0c\u6bd4\u5982 batch_size \u8bbe\u7f6e\u4e3a 64\uff0c\u800c\u4e00\u4e2a epoch\u53ea\u6709 100 \u4e2a\u6837\u672c\uff0c\u90a3\u4e48\u8bad\u7ec3\u7684\u65f6\u5019\u540e\u9762\u7684 36 \u4e2a\u5c31\u88ab\u6254\u6389\u4e86\uff0c\u5982\u679c\u4e3a False\uff08\u9ed8\u8ba4\uff09\uff0c\u90a3\u4e48\u4f1a\u7ee7\u7eed\u6b63\u5e38\u6267\u884c\uff0c\u53ea\u662f\u6700\u540e\u7684 batch_size \u4f1a\u5c0f\u4e00\u70b9\u3002\n", "    + timeout (numeric, optional): \u5982\u679c\u662f\u6b63\u6570\uff0c\u8868\u660e\u7b49\u5f85\u4ece worker \u8fdb\u7a0b\u4e2d\u6536\u96c6\u4e00\u4e2a batch \u7b49\u5f85\u7684\u65f6\u95f4\uff0c\u82e5\u8d85\u51fa\u8bbe\u5b9a\u7684\u65f6\u95f4\u8fd8\u6ca1\u6709\u6536\u96c6\u5230\uff0c\u90a3\u5c31\u4e0d\u6536\u96c6\u8fd9\u4e2a\u5185\u5bb9\u3002\u8fd9\u4e2a numeric \u5e94\u603b\u662f\u5927\u4e8e\u7b49\u4e8e0\uff0c\u9ed8\u8ba4\u4e3a0\u3002\n", "\n", "\u6211\u4eec\u91c7\u7528\u4ee5\u4e0a 2 \u6b65\u8fdb\u884c\u6570\u636e\u5904\u7406\uff0c\u4ee3\u7801\u5c55\u793a\u5982\u4e0b:\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def processing_data(data_path, height=224, width=224, batch_size=32,\n", "                    test_split=0.1):\n", "    \"\"\"\n", "    \u6570\u636e\u5904\u7406\u90e8\u5206\n", "    :param data_path: \u6570\u636e\u8def\u5f84\n", "    :param height:\u9ad8\u5ea6\n", "    :param width: \u5bbd\u5ea6\n", "    :param batch_size: \u6bcf\u6b21\u8bfb\u53d6\u56fe\u7247\u7684\u6570\u91cf\n", "    :param test_split: \u6d4b\u8bd5\u96c6\u5212\u5206\u6bd4\u4f8b\n", "    :return:\n", "    \"\"\"\n", "    transforms = T.Compose([\n", "        T.Resize((height, width)),\n", "        T.RandomHorizontalFlip(0.1),  # \u8fdb\u884c\u968f\u673a\u6c34\u5e73\u7ffb\u8f6c\n", "        T.RandomVerticalFlip(0.1),  # \u8fdb\u884c\u968f\u673a\u7ad6\u76f4\u7ffb\u8f6c\n", "        T.ToTensor(),  # \u8f6c\u5316\u4e3a\u5f20\u91cf\n", "        T.Normalize([0], [1]),  # \u5f52\u4e00\u5316\n", "    ])\n", "\n", "    dataset = ImageFolder(data_path, transform=transforms)\n", "    # \u5212\u5206\u6570\u636e\u96c6\n", "    train_size = int((1-test_split)*len(dataset))\n", "    test_size = len(dataset) - train_size\n", "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n", "    # \u521b\u5efa\u4e00\u4e2a DataLoader \u5bf9\u8c61\n", "    train_data_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n", "    valid_data_loader = DataLoader(test_dataset, batch_size=batch_size,shuffle=True)\n", "\n", "    return train_data_loader, valid_data_loader\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data_path = './datasets/5f680a696ec9b83bb0037081-momodel/data/image'\n", "train_data_loader, valid_data_loader = processing_data(data_path=data_path, height=160, width=160, batch_size=32)\n", "\n", "def show_tensor_img(img_tensor):\n", "    img = img_tensor[0].data.numpy()\n", "    img = np.swapaxes(img, 0, 2)\n", "    img = np.swapaxes(img, 0, 1)\n", "    img = np.array(img)\n", "    plot_image(img)\n", "\n", "for index, (x, labels) in enumerate(train_data_loader):\n", "    print(index, \"\\nfeature:\",x[0], \"\\nlabels:\",labels)\n", "    show_tensor_img(x)\n", "    break\n"]}, {"cell_type": "markdown", "metadata": {"toc-hr-collapsed": false}, "source": ["## 3. MTCNN\uff1a\u4eba\u8138\u68c0\u6d4b"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.1  MTCNN \u89e3\u8bfb\n", "\n", "\u53c2\u8003\u6587\u732e\uff1a\u300aJoint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks\u300b  \n", "\u6587\u732e\u4e0e\u4ee3\u7801\u5730\u5740\uff1ahttps://kpzhang93.github.io/MTCNN_face_detection_alignment/  \n", "  \n", "\u8bba\u6587\u7684\u4e3b\u8981\u8d21\u732e\uff1a  \n", "1\uff09**\u4e09\u9636\u6bb5\u7684\u7ea7\u8054\uff08cascaded\uff09\u67b6\u6784**  \n", "2\uff09**coarse-to-fine \u7684\u65b9\u5f0f**  \n", "3\uff09**new online hard sample mining \u7b56\u7565**  \n", "4\uff09**\u540c\u65f6\u8fdb\u884c\u4eba\u8138\u68c0\u6d4b\u548c\u4eba\u8138\u5bf9\u9f50**  \n", "5\uff09**state-of-the-art \u6027\u80fd**  \n", "\n", "<img src=\"https://imgbed.momodel.cn/20200918102724.png\"/>"]}, {"cell_type": "markdown", "metadata": {"toc-hr-collapsed": false}, "source": ["### 3.2 MTCNN \u7684\u4f7f\u7528\n", "\n", "\u8fd9\u91cc\u76f4\u63a5\u4f7f\u7528\u73b0\u6709\u7684\u8868\u73b0\u8f83\u597d\u7684 MTCNN \u7684\u4e09\u4e2a\u6743\u91cd\u6587\u4ef6\uff0c\u5b83\u4eec\u5df2\u7ecf\u4fdd\u5b58\u5728 `torch_py/MTCNN/weights` \u6587\u4ef6\u5939\u4e0b\uff0c\u8def\u5f84\u5982\u4e0b\uff1a\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pnet_path = \"./torch_py/MTCNN/weights/pnet.npy\"\n", "rnet_path = \"./torch_py/MTCNN/weights/rnet.npy\"\n", "onet_path = \"./torch_py/MTCNN/weights/onet.npy\"\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u901a\u8fc7\u642d\u5efa MTCNN \u7f51\u7edc\u5b9e\u73b0\u4eba\u8138\u68c0\u6d4b\uff08\u642d\u5efa\u6a21\u578b py \u6587\u4ef6\u5728 torch_py/MTCNN \u6587\u4ef6\u5939\uff09 "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["torch.set_num_threads(1)\n", "# \u8bfb\u53d6\u6d4b\u8bd5\u56fe\u7247\n", "img = Image.open(\"test.jpg\")\n", "# \u52a0\u8f7d\u6a21\u578b\u8fdb\u884c\u8bc6\u522b\u53e3\u7f69\u5e76\u7ed8\u5236\u65b9\u6846\n", "recognize = Recognition()\n", "draw = recognize.face_recognize(img)\n", "plot_image(draw)\n"]}, {"cell_type": "markdown", "metadata": {"toc-hr-collapsed": false}, "source": ["## 4. \u53e3\u7f69\u8bc6\u522b\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 4.1 \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b MobileNet"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u52a0\u8f7d MobileNet \u7684\u9884\u8bad\u7ec3\u6a21\u578b\u6743\n", "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n", "train_data_loader, valid_data_loader = processing_data(data_path=data_path, height=160, width=160, batch_size=32)\n", "modify_x, modify_y = torch.ones((32, 3, 160, 160)), torch.ones((32))\n", "\n", "epochs = 2\n", "model = MobileNetV1(classes=2).to(device)\n", "optimizer = optim.Adam(model.parameters(), lr=1e-3)  # \u4f18\u5316\u5668\n", "print('\u52a0\u8f7d\u5b8c\u6210...')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### **4.2 \u624b\u52a8\u8c03\u6574\u5b66\u4e60\u7387**(\u53ef\u8c03\u53c2\uff09\n", "\n", "\u5b66\u4e60\u7387\u7684\u624b\u52a8\u8bbe\u7f6e\u53ef\u4ee5\u4f7f\u6a21\u578b\u8bad\u7ec3\u66f4\u52a0\u9ad8\u6548\u3002  \n", "\u8fd9\u91cc\u6211\u4eec\u8bbe\u7f6e\u5f53\u6a21\u578b\u5728\u4e24\u8f6e\u8fed\u4ee3\u540e\uff0c\u51c6\u786e\u7387\u6ca1\u6709\u4e0a\u5347\uff0c\u5c31\u8c03\u6574\u5b66\u4e60\u7387\u3002"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u5b66\u4e60\u7387\u4e0b\u964d\u7684\u65b9\u5f0f\uff0cacc\u4e09\u6b21\u4e0d\u4e0b\u964d\u5c31\u4e0b\u964d\u5b66\u4e60\u7387\u7ee7\u7eed\u8bad\u7ec3\uff0c\u8870\u51cf\u5b66\u4e60\u7387\n", "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n", "                                                 'max',\n", "                                                 factor=0.5,\n", "                                                 patience=2)\n", "# \u635f\u5931\u51fd\u6570\n", "criterion = nn.CrossEntropyLoss()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 4.3 \u8bad\u7ec3\u6a21\u578b"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["best_loss = 1e9\n", "best_model_weights = copy.deepcopy(model.state_dict())\n", "loss_list = []  # \u5b58\u50a8\u635f\u5931\u51fd\u6570\u503c\n", "for epoch in range(epochs):\n", "    model.train()\n", "\n", "    for batch_idx, (x, y) in tqdm(enumerate(train_data_loader, 1)):\n", "        x = x.to(device)\n", "        y = y.to(device)\n", "        pred_y = model(x)\n", "\n", "        # print(pred_y.shape)\n", "        # print(y.shape)\n", "\n", "        loss = criterion(pred_y, y)\n", "        optimizer.zero_grad()\n", "        loss.backward()\n", "        optimizer.step()\n", "\n", "        if loss < best_loss:\n", "            best_model_weights = copy.deepcopy(model.state_dict())\n", "            best_loss = loss\n", "\n", "        loss_list.append(loss)\n", "\n", "    print('step:' + str(epoch + 1) + '/' + str(epochs) + ' || Total Loss: %.4f' % (loss))\n", "torch.save(model.state_dict(), './results/temp.pth')\n", "print('Finish Training.')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 4.4 \u5c55\u793a\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(loss_list,label = \"loss\")\n", "plt.legend()\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 4.5 \u68c0\u6d4b\u56fe\u7247\u4e2d\u4eba\u6570\u53ca\u6234\u53e3\u7f69\u7684\u4eba\u6570"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["img = Image.open(\"test.jpg\")\n", "detector = FaceDetector()\n", "recognize = Recognition(model_path='results/temp.pth')\n", "draw, all_num, mask_nums = recognize.mask_recognize(img)\n", "plt.imshow(draw)\n", "plt.show()\n", "print(\"all_num:\", all_num, \"mask_num\", mask_nums)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5.\u4f5c\u4e1a"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 5.1 \u8bad\u7ec3\u6a21\u578b"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u53e3\u7f69\u4f69\u6234\u68c0\u6d4b\u6a21\u578b\u8bad\u7ec3\u6d41\u7a0b, \u5305\u542b\u6570\u636e\u5904\u7406\u3001\u521b\u5efa\u6a21\u578b\u3001\u8bad\u7ec3\u6a21\u578b\u3001\u6a21\u578b\u4fdd\u5b58\u3001\u8bc4\u4ef7\u6a21\u578b\u7b49\u3002\u8bad\u7ec3\u6a21\u578b\u53ef\u4ee5\u53c2\u8003\u7b2c 4.3 \u90e8\u5206\u8bad\u7ec3\u6a21\u578b\u4ee3\u7801  \n", "\u5982\u679c\u5bf9\u8bad\u7ec3\u51fa\u6765\u7684\u6a21\u578b\u4e0d\u6ee1\u610f, \u4f60\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u6a21\u578b\u7684\u53c2\u6570\u7b49\u65b9\u6cd5\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b, \u76f4\u81f3\u8bad\u7ec3\u51fa\u4f60\u6ee1\u610f\u7684\u6a21\u578b\u3002  \n", "\u5982\u679c\u4f60\u5bf9\u81ea\u5df1\u8bad\u7ec3\u51fa\u6765\u7684\u6a21\u578b\u975e\u5e38\u6ee1\u610f, \u5219\u53ef\u4ee5\u63d0\u4ea4\u4f5c\u4e1a!  \n", "\n", "\u6ce8\u610f\uff1a\n", "\n", "1. \u4f60\u53ef\u4ee5\u5728\u6211\u4eec\u51c6\u597d\u7684\u63a5\u53e3\u4e2d\u5b9e\u73b0\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u82e5\u4f7f\u7528\u53ef\u4ee5\u4fee\u6539\u51fd\u6570\u63a5\u53e3\uff09\uff0c\u4e5f\u53ef\u4ee5\u81ea\u5df1\u5b9e\u73b0\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\n", "2. \u5199\u597d\u4ee3\u7801\u540e\u53ef\u4ee5\u5728 Py \u6587\u4ef6\u4e2d\u4f7f\u7528 [\u79bb\u7ebf\u4efb\u52a1](https://momodel.cn/docs/#/zh-cn/%E5%9C%A8GPU%E6%88%96CPU%E8%B5%84%E6%BA%90%E4%B8%8A%E8%AE%AD%E7%BB%83%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B) \u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u3002\n", "3. **\u4f7f\u7528\u79bb\u7ebf\u8bad\u7ec3\u6a21\u578b\u5fc5\u987b\u4fdd\u5b58\u5728 results \u6587\u4ef6\u5939**\u3002    \n", "4. \u5c06\u81ea\u5df1\u8ba4\u4e3a\u6700\u4f73\u6a21\u578b\u4fdd\u5b58\u5728 result \u6587\u4ef6\u5939\uff0c\u5176\u4f59\u6a21\u578b\u5907\u4efd\u5728\u9879\u76ee\u4e2d\u5176\u5b83\u6587\u4ef6\u5939\uff0c\u65b9\u4fbf\u60a8\u52a0\u5feb\u6d4b\u8bd5\u901a\u8fc7\u3002\n"]}, {"cell_type": "markdown", "metadata": {"inputHidden": false}, "source": ["===========================================  \u5b9e\u73b0\u81ea\u5df1\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4ee3\u7801\u7b54\u9898\u533a\u57df  ===========================================\n", "\n", "\u53cc\u51fb\u4e0b\u65b9\u533a\u57df\u5f00\u59cb\u7f16\u5199  **\u6570\u636e\u5904\u7406**\u3001**\u521b\u5efa\u6a21\u578b**\u3001**\u8bad\u7ec3\u6a21\u578b**\u3001**\u4fdd\u5b58\u6a21\u578b**  \u548c  **\u8bc4\u4f30\u6a21\u578b**  \u7b49\u90e8\u5206\u7684\u4ee3\u7801\uff0c\u8bf7\u52ff\u5728\u522b\u7684\u4f4d\u7f6e\u4f5c\u7b54"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# 1.\u52a0\u8f7d\u6570\u636e\u5e76\u8fdb\u884c\u6570\u636e\u5904\u7406\n", "\n", "# 2.\u5982\u679c\u6709\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5219\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\uff1b\u5982\u679c\u6ca1\u6709\u5219\u4e0d\u9700\u8981\u52a0\u8f7d\n", "\n", "# 3.\u521b\u5efa\u6a21\u578b\u548c\u8bad\u7ec3\u6a21\u578b\uff0c\u8bad\u7ec3\u6a21\u578b\u65f6\u5c3d\u91cf\u5c06\u6a21\u578b\u4fdd\u5b58\u5728 results \u6587\u4ef6\u5939\n", "\n", "# 4.\u8bc4\u4f30\u6a21\u578b\uff0c\u5c06\u81ea\u5df1\u8ba4\u4e3a\u6700\u4f73\u6a21\u578b\u4fdd\u5b58\u5728 result \u6587\u4ef6\u5939\uff0c\u5176\u4f59\u6a21\u578b\u5907\u4efd\u5728\u9879\u76ee\u4e2d\u5176\u5b83\u6587\u4ef6\u5939\uff0c\u65b9\u4fbf\u60a8\u52a0\u5feb\u6d4b\u8bd5\u901a\u8fc7\u3002\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 5.2 \u63d0\u4ea4\u4f5c\u4e1a"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**\u4f5c\u4e1a\u8981\u6c42\u53ca\u6ce8\u610f\u4e8b\u9879**\uff1a    \n", "\n", "1.\u4f7f\u7528\u4e0a\u8ff0\u5b66\u5230\u7684\u65b9\u6cd5\uff0c\u8bad\u7ec3\u81ea\u5df1\u7684\u53e3\u7f69\u8bc6\u522b\u6a21\u578b\uff0c\u5c3d\u53ef\u80fd\u63d0\u9ad8\u51c6\u786e\u5ea6\u3002\u5c06\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u4fdd\u5b58\u5728 results \u6587\u4ef6\u5939\u4e0b\u3002             \n", "2.\u70b9\u51fb\u5de6\u4fa7\u680f\u63d0\u4ea4\u4f5c\u4e1a\u540e\u70b9\u51fb\u3010\u751f\u6210\u6587\u4ef6\u3011\u5219\u9700\u8981\u52fe\u9009\u4e0e\u9884\u6d4b predict() \u51fd\u6570\u7684 cell\u76f8\u5173\u7684\u5176\u5b83cell \uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u6210\u4e3a main.py \u6587\u4ef6\u3002                       \n", "3.\u8bf7\u5bfc\u5165\u5fc5\u8981\u7684\u5305\u548c\u7b2c\u4e09\u65b9\u5e93\u4ee5\u53ca\u8be5\u6a21\u578b\u6240\u4f9d\u8d56\u7684 py \u6587\u4ef6 (\u5305\u62ec\u6b64\u6587\u4ef6\u4e2d\u66fe\u7ecf\u5bfc\u5165\u8fc7\u7684)\u3002             \n", "4.\u8bf7\u52a0\u8f7d\u4f60\u8ba4\u4e3a\u8bad\u7ec3\u6700\u4f73\u7684\u6a21\u578b\uff0c\u5373\u8bf7\u6309\u8981\u6c42\u586b\u5199\u6a21\u578b\u8def\u5f84\u3002              \n", "5.predict() \u51fd\u6570\u7684\u8f93\u5165\u8f93\u51fa\u53ca\u51fd\u6570\u540d\u79f0\u8bf7\u4e0d\u8981\u6539\u52a8\u3002\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["===========================================  **\u6a21\u578b\u9884\u6d4b\u4ee3\u7801\u7b54\u9898\u533a\u57df**  ===========================================  \n", "\u5728\u4e0b\u65b9\u7684\u4ee3\u7801\u5757\u4e2d\u7f16\u5199 **\u6a21\u578b\u9884\u6d4b** \u90e8\u5206\u7684\u4ee3\u7801\uff0c\u8bf7\u52ff\u5728\u522b\u7684\u4f4d\u7f6e\u4f5c\u7b54"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "select": true}, "outputs": [], "source": ["from torch_py.Utils import plot_image\n", "from torch_py.MTCNN.detector import FaceDetector\n", "from torch_py.MobileNetV1 import MobileNetV1\n", "from torch_py.FaceRec import Recognition\n", "from torch_py.FaceRec import Recognition\n", "from PIL import Image\n", "import cv2\n", "\n", "# -------------------------- \u8bf7\u52a0\u8f7d\u60a8\u6700\u6ee1\u610f\u7684\u6a21\u578b ---------------------------\n", "# \u52a0\u8f7d\u6a21\u578b(\u8bf7\u52a0\u8f7d\u4f60\u8ba4\u4e3a\u7684\u6700\u4f73\u6a21\u578b)\n", "# \u52a0\u8f7d\u6a21\u578b,\u52a0\u8f7d\u8bf7\u6ce8\u610f model_path \u662f\u76f8\u5bf9\u8def\u5f84, \u4e0e\u5f53\u524d\u6587\u4ef6\u540c\u7ea7\u3002\n", "# \u5982\u679c\u4f60\u7684\u6a21\u578b\u662f\u5728 results \u6587\u4ef6\u5939\u4e0b\u7684 dnn.h5 \u6a21\u578b\uff0c\u5219 model_path = 'results/temp.pth'\n", "model_path = None\n", "# ---------------------------------------------------------------------------\n", "\n", "def predict(img):\n", "    \"\"\"\n", "    \u52a0\u8f7d\u6a21\u578b\u548c\u6a21\u578b\u9884\u6d4b\n", "    :param img: cv2.imread \u56fe\u50cf\n", "    :return: \u9884\u6d4b\u7684\u56fe\u7247\u4e2d\u7684\u603b\u4eba\u6570\u3001\u5176\u4e2d\u4f69\u6234\u53e3\u7f69\u7684\u4eba\u6570\n", "    \"\"\"\n", "    # -------------------------- \u5b9e\u73b0\u6a21\u578b\u9884\u6d4b\u90e8\u5206\u7684\u4ee3\u7801 ---------------------------\n", "    # \u5c06 cv2.imread \u56fe\u50cf\u8f6c\u5316\u4e3a PIL.Image \u56fe\u50cf\uff0c\u7528\u6765\u517c\u5bb9\u6d4b\u8bd5\u8f93\u5165\u7684 cv2 \u8bfb\u53d6\u7684\u56fe\u50cf\uff08\u52ff\u5220\uff01\uff01\uff01\uff09\n", "    # cv2.imread \u8bfb\u53d6\u56fe\u50cf\u7684\u7c7b\u578b\u662f numpy.ndarray\n", "    # PIL.Image.open \u8bfb\u53d6\u56fe\u50cf\u7684\u7c7b\u578b\u662f PIL.JpegImagePlugin.JpegImageFile\n", "    if isinstance(img, np.ndarray):\n", "        # \u8f6c\u5316\u4e3a PIL.JpegImagePlugin.JpegImageFile \u7c7b\u578b\n", "        img = Image.fromarray(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n", "\n", "    recognize = Recognition(model_path)\n", "    img, all_num, mask_num = recognize.mask_recognize(img)\n", "    # -------------------------------------------------------------------------\n", "    return all_num,mask_num\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u8f93\u5165\u56fe\u7247\u8def\u5f84\u548c\u540d\u79f0\n", "img = cv2.imread(\"test1.jpg\")\n", "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n", "all_num, mask_num = predict(img)\n", "# \u6253\u5370\u9884\u6d4b\u8be5\u5f20\u56fe\u7247\u4e2d\u603b\u4eba\u6570\u4ee5\u53ca\u6234\u53e3\u7f69\u7684\u4eba\u6570\n", "print(all_num, mask_num)\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.5"}}, "nbformat": 4, "nbformat_minor": 4}